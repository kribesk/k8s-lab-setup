
## Network boot techniques

_(Section by Boris Kirikov)_

When building cluster automating bare metal provisioning is absolutely essential. That means nodes can be provisioned with latest OS
images with preinstalled cluster software, standard and custom configuration, booted. So that they'll automatically join cluster with
minimal amount of manual operations.

Our aim is to automate all the process after startup. Farther automation will require access to BIOS settings (like enabling Wake-On-LAN
and boot device priority) and does not make that much simplification.

### PXE standard {#sec:pxe}

PXE or Preboot eXecution Environment standard method for network boot. PXE is often a feature built-in network adapter. PXE is simple
and very common: it is embedded in vast majority of modern network cards. Also MB316 computers have PXE feature.

PXE does not allow much freedom in boot process and used protocols, but it is still enough to boot everything. One of the reasons is that
PXE like MBR have limit on size of firmware (32 KB). 

Netboot is based on BOOTP/DHCP protocol:

  1. Computer is started up and BIOS decides to initiate netboot
  2. Control is passed to PXE firmware of network card
  3. Network card tries to get configuration from DHCP server
  4. If there is DHCP server on the network, network card gets configuration
  5. In addition to settings like IP, mask, gateway and DNS, DHCP protocol can provide number of additional fields and "vendor specific fields"
  6. The interesting fields are `filename` and `next-server`
  7. PXE firmware connects via TFTP protocol to `next-server`, downloads `filename` and executes it

This means that to set up netboot we need to start reconfigure DHCP server and start TFTP server on our network.

The next file that is executed (`filename`) can be everything, it is not limited to OS image. For example it can be more powerful netboot
software that will allow perform more sophisticated boot process. Examples of such software are pxelinux, GRUB and iPXE.

### iPXE

"iPXE is the leading open source network boot firmware. It provides a full PXE implementation enhanced with additional features". 

That
means it is still PXE and operates in the same way. If we will use it like that, iPXE will connect to DHCP, get configuration, download
and iPXE again. And again, and again. This loop should be broken. There are two options for that: set up DHCP server so that it it 
provides different configuration for iPXE clients or modify iPXE (rebuild with embedded script) so that it won't boot from DHCP settings.

### iPXE as a shell

Firmware implementations of PXE are straightforward: they just perform described algorithm. But iPXE is different. iPXE has shell and
scripting language that allows customizing boot process, using different protocols and displaying menus.

iPXE script can be provided by DHCP server in `filename` field. Now this address can be not on TFTP server, but it can be web server.
More generally, it can be not a static file but generated by server in response to iPXE request.

iPXE script start with shebang `#!ipxe` and after that come lines of commands. The most important command is `chain <file>` that chainloads
to specified file, that can be accessed through supported protocol, like HTTP, FTP, iSCSI, FCoE and others. `dhcp` is used to contact
DHCP server and obtain configuration. Scripting allows also comments, flow control with goto and short-circuit logic and variables.
Full list of command available in [command reference](http://ipxe.org/cmd).

So the main idea is that iPXE with good boot script can boot everything we need.

### Matchbox

Matchbox is set of tools and services for netboot and provision of CoreOS, host OS for kubernetes.

Matchbox services are packed into docker (and rkt) containers. The main service provides iPXE http server, that generates iPXE boot scripts
and also ignition provision configuration for CoreOS (described in CoreOS section) and metadata. That is everything we need to boot from
iPXE. 

For PXE stage, matchbox suggests using dnsmasq simple dhcp/dns/tftp server packed also into docker container. 

As soon as both used services are packed into containers, they can run inside cluster after first nodes are provisioned.

Matchbox configuration is based on groups and profiles. If node matches group's selector, group profile is used to build configuration
files. Profile defines which images are used to boot host and what config templates are used to generate configuration files.

So the overall process is:

  1. Host is powered on, BIOS performed checks and passes control to network card's PXE
  2. PXE requests configuration from DHCP dnsmasq server (running in standalone docker or already in cluster)
  3. Server determines that it is not iPXE and returns its address as `next-server` and `undionly.kpxe` as `filename`
  4. PXE loads iPXE from dnsmasq tftp and runs it
  5. iPXE requests configuration from DHCP dnsmasq server
  6. Server determines that it is now iPXE and returns address of matchbox iPXE endpoint: `filename` = `http://<matchbox>:8080/ipxe?mac=${mac}`
  7. iPXE downloads script and chains to it
  8. Script provide address of image and additional parameters, like ignition config url
  9. CoreOS image is downloaded, control is passed to CoreOS ignition system
  10. Ignition config is downloaded and used to correctly boot CoreOS, set everything up and connect to cluster (or start it, if belongs 
      to controller group in matchbox config)
     